{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spotify Songs Analysis - Predictive Modeling\n",
        "\n",
        "## 1. Setup and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Add src to path for custom modules\n",
        "sys.path.append('../src')\n",
        "from modeling import make_baseline_model, train_and_evaluate, save_model\n",
        "\n",
        "# Set up paths\n",
        "DATA_DIR = Path('../data')\n",
        "CLEAN_DATA_PATH = DATA_DIR / 'clean_spotify.csv'\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Set larger font sizes for better readability\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "plt.rcParams['axes.titlesize'] = 14\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load cleaned data\n",
        "df = pd.read_csv(CLEAN_DATA_PATH)\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(\"\\nFirst 3 rows:\")\n",
        "display(df.head(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Target Variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create popularity categories (target variable) if not already created by notebook 2\n",
        "# 0 = Low (<40), 1 = Medium (40-60), 2 = High (>60)\n",
        "if 'popularity_category' not in df.columns:\n",
        "    df['popularity_category'] = pd.cut(\n",
        "        df['popularity'], \n",
        "        bins=[-1, 40, 60, 101], \n",
        "        labels=[0, 1, 2]\n",
        "    ).astype(int)\n",
        "    print(\"Created popularity_category column\")\n",
        "else:\n",
        "    # Convert to integer if it's already there (might be from notebook 2)\n",
        "    if df['popularity_category'].dtype == 'object' or df['popularity_category'].dtype.name == 'category':\n",
        "        # Convert string labels to integers if needed\n",
        "        label_map = {'Low': 0, 'Medium': 1, 'High': 2}\n",
        "        if df['popularity_category'].dtype.name == 'category':\n",
        "            # Get the categories\n",
        "            cats = df['popularity_category'].cat.categories.tolist()\n",
        "            # If they're strings, map them\n",
        "            if any(isinstance(c, str) for c in cats):\n",
        "                df['popularity_category'] = df['popularity_category'].astype(str).map(label_map)\n",
        "            else:\n",
        "                df['popularity_category'] = df['popularity_category'].astype(int)\n",
        "        else:\n",
        "            df['popularity_category'] = df['popularity_category'].map(label_map).fillna(df['popularity_category']).astype(int)\n",
        "        print(\"Converted popularity_category to integer format\")\n",
        "    else:\n",
        "        df['popularity_category'] = df['popularity_category'].astype(int)\n",
        "        print(\"Using existing popularity_category column\")\n",
        "\n",
        "print(\"\\nPopularity category distribution:\")\n",
        "print(df['popularity_category'].value_counts().sort_index())\n",
        "\n",
        "# Check for any NaN values\n",
        "if df['popularity_category'].isna().any():\n",
        "    print(\"\\nWarning: Found NaN values in popularity_category. Dropping these rows.\")\n",
        "    df = df.dropna(subset=['popularity_category'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select features for modeling\n",
        "feature_columns = ['danceability', 'energy', 'loudness', 'acousticness', \n",
        "                   'instrumentalness', 'liveness', 'valence', 'tempo']\n",
        "\n",
        "# Use only features that exist in the dataframe\n",
        "available_features = [f for f in feature_columns if f in df.columns]\n",
        "\n",
        "print(f\"Features to use: {available_features}\")\n",
        "print(f\"\\nFeatures missing: {set(feature_columns) - set(available_features)}\")\n",
        "\n",
        "# Create feature matrix and target\n",
        "X = df[available_features].copy()\n",
        "y = df['popularity_category'].copy()\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"Target distribution:\\n{y.value_counts().sort_index()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "print(f\"\\nTraining set target distribution:\\n{y_train.value_counts().sort_index()}\")\n",
        "print(f\"\\nTest set target distribution:\\n{y_test.value_counts().sort_index()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training and Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Model 1: Logistic Regression\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL 1: Logistic Regression with StandardScaler\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "lr_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "lr_pipeline.fit(X_train, y_train)\n",
        "y_pred_lr = lr_pipeline.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
        "print(f\"F1 Score (macro): {f1_score(y_test, y_pred_lr, average='macro'):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_lr, target_names=['Low', 'Medium', 'High']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model 2: Random Forest\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL 2: Random Forest Classifier\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
        "print(f\"F1 Score (macro): {f1_score(y_test, y_pred_rf, average='macro'):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf, target_names=['Low', 'Medium', 'High']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: XGBoost (if available)\n",
        "xgb_available = False\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    print(\"=\" * 60)\n",
        "    print(\"MODEL 3: XGBoost Classifier\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    xgb_model = xgb.XGBClassifier(random_state=42, n_jobs=-1, eval_metric='mlogloss')\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "    y_pred_xgb = xgb_model.predict(X_test)\n",
        "    \n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
        "    print(f\"F1 Score (macro): {f1_score(y_test, y_pred_xgb, average='macro'):.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred_xgb, target_names=['Low', 'Medium', 'High']))\n",
        "    xgb_available = True\n",
        "except ImportError:\n",
        "    print(\"XGBoost not available. Skipping XGBoost model.\")\n",
        "    xgb_available = False\n",
        "except Exception as e:\n",
        "    print(f\"Error with XGBoost: {e}\")\n",
        "    xgb_available = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Confusion Matrices Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Logistic Regression Confusion Matrix\n",
        "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
        "            xticklabels=['Low', 'Medium', 'High'], \n",
        "            yticklabels=['Low', 'Medium', 'High'])\n",
        "axes[0].set_title('Logistic Regression Confusion Matrix')\n",
        "axes[0].set_ylabel('True Label')\n",
        "axes[0].set_xlabel('Predicted Label')\n",
        "\n",
        "# Random Forest Confusion Matrix\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
        "            xticklabels=['Low', 'Medium', 'High'], \n",
        "            yticklabels=['Low', 'Medium', 'High'])\n",
        "axes[1].set_title('Random Forest Confusion Matrix')\n",
        "axes[1].set_ylabel('True Label')\n",
        "axes[1].set_xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/figures/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance from Random Forest\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': available_features,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Feature Importance (Random Forest):\")\n",
        "display(feature_importance)\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(len(feature_importance)), feature_importance['importance'], color='steelblue')\n",
        "plt.yticks(range(len(feature_importance)), feature_importance['feature'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance (Random Forest)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig('../reports/figures/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Comparison and Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare models\n",
        "results = pd.DataFrame({\n",
        "    'Model': ['Logistic Regression', 'Random Forest'],\n",
        "    'Accuracy': [accuracy_score(y_test, y_pred_lr), accuracy_score(y_test, y_pred_rf)],\n",
        "    'F1 Score (macro)': [f1_score(y_test, y_pred_lr, average='macro'), \n",
        "                         f1_score(y_test, y_pred_rf, average='macro')]\n",
        "})\n",
        "\n",
        "if xgb_available:\n",
        "    results = pd.concat([results, pd.DataFrame({\n",
        "        'Model': ['XGBoost'],\n",
        "        'Accuracy': [accuracy_score(y_test, y_pred_xgb)],\n",
        "        'F1 Score (macro)': [f1_score(y_test, y_pred_xgb, average='macro')]\n",
        "    })], ignore_index=True)\n",
        "\n",
        "print(\"Model Comparison:\")\n",
        "display(results.sort_values('Accuracy', ascending=False))\n",
        "\n",
        "# Select best model based on F1 score\n",
        "best_model_name = results.loc[results['F1 Score (macro)'].idxmax(), 'Model']\n",
        "print(f\"\\nBest model (by F1 Score): {best_model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Best Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the best model (Random Forest for now, as it typically performs well)\n",
        "from pathlib import Path\n",
        "import joblib\n",
        "\n",
        "models_dir = Path('../models')\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save Random Forest model (you can change this to save the best model dynamically)\n",
        "best_model = rf_model\n",
        "model_path = models_dir / 'best_model.joblib'\n",
        "joblib.dump(best_model, model_path)\n",
        "print(f\"Best model saved to: {model_path}\")\n",
        "\n",
        "# Also save the feature names for later use\n",
        "feature_names_path = models_dir / 'feature_names.joblib'\n",
        "joblib.dump(available_features, feature_names_path)\n",
        "print(f\"Feature names saved to: {feature_names_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary and Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"MODELING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nüìä MODEL PERFORMANCE:\")\n",
        "print(\"-\" * 40)\n",
        "for idx, row in results.iterrows():\n",
        "    print(f\"{row['Model']}:\")\n",
        "    print(f\"  ‚Ä¢ Accuracy: {row['Accuracy']:.4f}\")\n",
        "    print(f\"  ‚Ä¢ F1 Score (macro): {row['F1 Score (macro)']:.4f}\")\n",
        "\n",
        "print(f\"\\nüîç TOP FEATURES (Random Forest):\")\n",
        "print(\"-\" * 40)\n",
        "for idx, row in feature_importance.head(5).iterrows():\n",
        "    print(f\"  ‚Ä¢ {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "print(f\"\\nüíæ SAVED MODEL:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"  ‚Ä¢ Model: {model_path}\")\n",
        "print(f\"  ‚Ä¢ Features: {feature_names_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Modeling completed successfully!\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
